{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from RBM.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zda/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/home/zda/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/zda/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/zda/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/zda/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import Ipynb_importer\n",
    "from RBM import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDNE(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(SDNE, self).__init__()\n",
    "        self.is_variables_init = False\n",
    "        self.config=config  #config =dict\n",
    "        \n",
    "        ######define variables ###\n",
    "        self.layers=len(config.struct)  #struct:[-1(1),100....]\n",
    "        self.struct=config.struct\n",
    "        self.sparse_dot=config.sparse_dot   #config.sparse_dot 不存在啊\n",
    "        struct=self.struct\n",
    "        \n",
    "        ###\n",
    "        self.WE1=Parameter(torch.FloatTensor(np.random.normal(size=(struct[0],struct[1]))))\n",
    "        self.be1=Parameter(torch.FloatTensor(np.random.normal(size=(struct[1]))))\n",
    "        \n",
    "        self.WD1=Parameter(torch.from_numpy(np.random.normal(size=(struct[1],struct[0]))))\n",
    "        self.bd1=Parameter(torch.from_numpy(np.random.normal(size=(struct[0]))))\n",
    "        \n",
    "#         self.W={}\n",
    "#         self.b={}\n",
    "#         for i in range(self.layers-1):\n",
    "#             name=\"encoder\"+str(i)\n",
    "#             self.W[name]=Parameter(torch.from_numpy(np.random.normal(size=(struct[i],struct[i+1]))))\n",
    "#             self.b[name]=Parameter(torch.zeros(struct[i+1]))\n",
    "#         struct.reverse()\n",
    "#         for i in range(self.layers-1):\n",
    "#             name=\"decoder\"+str(i)\n",
    "#             self.W[name]=Parameter(torch.from_numpy(np.random.normal(size=(struct[i],struct[i+1]))))\n",
    "#             self.b[name]=Parameter(torch.zeros(struct[i+1]))\n",
    "#         struct.reverse()\n",
    "        ###########################\n",
    "        \n",
    "        ############define input###\n",
    "    def forward(self,data,adj,config):\n",
    "        self.X=data\n",
    "#         self.do_variables_init(data)\n",
    "        self._make_compute_graph()\n",
    "        self.loss=self._make_loss(config,adj)\n",
    "        return self.loss\n",
    "        \n",
    "        \n",
    "    def _make_compute_graph(self):\n",
    "        def encoder(X):\n",
    "            for i in range(self.layers-1):\n",
    "                name=\"encoder\"+str(i)\n",
    "                X=F.sigmoid(torch.matmul(X,self.WE1+self.be1))\n",
    "            return X\n",
    "\n",
    "        def encoder_sp(X):\n",
    "            for i in range(self.layers-1):\n",
    "                name=\"encoder\"+str(i)\n",
    "                if i ==0:\n",
    "                    X=F.sigmoid(torch.sparse.mm(X,self.WE1)+self.be1)\n",
    "                else:\n",
    "                    X=F.sigmoid(torch.matmul(X,self.WE1)+self.be1)\n",
    "            return X\n",
    "\n",
    "        def decoder(X):\n",
    "            for i in range(self.layers-1):\n",
    "                name=\"decoder\"+str(i)\n",
    "                X=F.sigmoid(torch.matmul(X,self.WD1)+self.bd1)\n",
    "            return X\n",
    "\n",
    "        if self.sparse_dot:\n",
    "            self.H=encoder_sp(self.X_sp)\n",
    "        else:\n",
    "            self.H=encoder(self.X)\n",
    "\n",
    "        self.X_reconstruct=decoder(self.H)\n",
    "\n",
    "####\n",
    "#     def _make_compute_graph(self):\n",
    "#         def encoder(X):\n",
    "#             for i in range(self.layers-1):\n",
    "#                 name=\"encoder\"+str(i)\n",
    "#                 X=F.sigmoid(torch.matmul(X,self.W[name]+self.b[name]))\n",
    "#             return X\n",
    "\n",
    "#         def encoder_sp(X):\n",
    "#             for i in range(self.layers-1):\n",
    "#                 name=\"encoder\"+str(i)\n",
    "#                 if i ==0:\n",
    "#                     X=F.sigmoid(torch.sparse.mm(X,self.W[name])+self.b[name])\n",
    "#                 else:\n",
    "#                     X=F.sigmoid(torch.matmul(X,self.W[name])+self.b[name])\n",
    "#             return X\n",
    "\n",
    "#         def decoder(X):\n",
    "#             for i in range(self.layers-1):\n",
    "#                 name=\"decoder\"+str(i)\n",
    "#                 X=F.sigmoid(torch.matmul(X,self.W[name])+self.b[name])\n",
    "#             return X\n",
    "\n",
    "#         if self.sparse_dot:\n",
    "#             self.H=encoder_sp(self.X_sp)\n",
    "#         else:\n",
    "#             self.H=encoder(self.X)\n",
    "\n",
    "#         self.X_reconstruct=decoder(self.H)\n",
    "\n",
    "    ####\n",
    "    def _make_loss(self,config,adj_mini_batch):\n",
    "        self.adjacent_matriX=adj_mini_batch\n",
    "        def get_1st_loss_link_sample(self, Y1, Y2):\n",
    "            return torch.sum(torch.pow(Y1 - Y2, 2))\n",
    "        def get_1st_loss(H, adj_mini_batch):\n",
    "            D = torch.diag(torch.sum(adj_mini_batch,1))\n",
    "            L = D - adj_mini_batch ## L is laplation-matriX\n",
    "            return 2*torch.trace(torch.matmul(torch.matmul(torch.transpose(H,0,1),L),H))\n",
    "\n",
    "\n",
    "        def get_2nd_loss(X, newX, beta):\n",
    "            B = X * (beta - 1) + 1\n",
    "            return torch.sum(torch.pow((newX - X)* B, 2))\n",
    "\n",
    "#             def get_reg_loss(weight, biases):\n",
    "#                 ret = torch.sum([sum(w**2)/() for w in weight.values()])\n",
    "#                 ret = ret + tf.add_n([tf.nn.l1_loss(b) for b in biases.values()])\n",
    "#                 return ret\n",
    "        self.loss_2nd = get_2nd_loss(self.X, self.X_reconstruct, config.beta)\n",
    "        self.loss_1st = get_1st_loss(self.H, self.adjacent_matriX)\n",
    "#         self.loss_xxx = tf.reduce_sum(tf.pow(self.X_reconstruct,2)) \n",
    "    # we don't need the regularizer term, since we have nagetive sampling.\n",
    "#         self.loss_reg = get_reg_loss(self.W, self.b) \n",
    "        return config.gamma * self.loss_1st + config.alpha * self.loss_2nd #+ config.reg * self.loss_reg\n",
    "\n",
    "        \n",
    "    def do_variables_init(self,data):\n",
    "        shape=self.struct\n",
    "        myRBMs=[]\n",
    "        for i in range(len(shape)-1):\n",
    "            myRBM=rbm([shape[i],shape[i+1]],{\"batch_size\": self.config.dbn_batch_size,\n",
    "                                             \"learning_rate\":self.config.dbn_learning_rate})\n",
    "            myRBMs.append(myRBM)\n",
    "            for epoch in range(self.config.dbn_epochs):\n",
    "                error=0\n",
    "                for batch in range(0,data[0].N,self.config.dbn_batch_size):\n",
    "                    mini_batch=data[0].sample(self.config.dbn_batch_size).X\n",
    "                    for k in range(len(myRBMs)-1):\n",
    "                        mini_batch=myRBMs[k].getH(mini_batch)\n",
    "                    error+=myRBM(mini_batch)\n",
    "                print(\"rbm epochs:\", epoch, \"error : \", error)\n",
    "                W, bv, bh = myRBM.W,myRBM.bv,myRBM.bh\n",
    "                name = \"encoder\" + str(i)\n",
    "                self.WE1.data= W\n",
    "                self.be1.data=bh\n",
    "                name = \"decoder\" + str(self.layers - i - 2)\n",
    "                self.WD1.data=W.transpose(0,1)\n",
    "                self.bd1.data=bv\n",
    "            self.is_Init=True\n",
    "#     def get_embedding(self,data):\n",
    "#         return self.H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

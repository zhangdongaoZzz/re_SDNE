{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import pdb\n",
    "\n",
    "class Dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "\n",
    "def getSimilarity(result):\n",
    "    print( \"getting similarity...\")\n",
    "    return np.dot(result, result.T)\n",
    "    \n",
    "def check_reconstruction(embedding, graph_data, check_index):\n",
    "    def get_precisionK(embedding, data, max_index):\n",
    "        print (\"get precisionK...\")\n",
    "        similarity = getSimilarity(embedding).reshape(-1)\n",
    "        sortedInd = np.argsort(similarity)\n",
    "        cur = 0\n",
    "        count = 0\n",
    "        precisionK = []\n",
    "        sortedInd = sortedInd[::-1]\n",
    "        for ind in sortedInd:\n",
    "            x = ind / data.N\n",
    "            y = ind % data.N\n",
    "            count += 1\n",
    "            if (data.adj_matrix[x].toarray()[0][y] == 1 or x == y):\n",
    "                cur += 1 \n",
    "            precisionK.append(1.0 * cur / count)\n",
    "            if count > max_index:\n",
    "                break\n",
    "        return precisionK\n",
    "        \n",
    "    precisionK = get_precisionK(embedding, graph_data, np.max(check_index))\n",
    "    ret = []\n",
    "    for index in check_index:\n",
    "        print( \"precisonK[%d] %.2f\" % (index, precisionK[index - 1]))\n",
    "        ret.append(precisionK[index - 1])\n",
    "    return ret\n",
    "\n",
    "def check_link_prediction(embedding, train_graph_data, origin_graph_data, check_index):\n",
    "    def get_precisionK(embedding, train_graph_data, origin_graph_data, max_index):\n",
    "        print (\"get precisionK...\")\n",
    "        similarity = getSimilarity(embedding).reshape(-1)\n",
    "        sortedInd = np.argsort(similarity)\n",
    "        cur = 0\n",
    "        count = 0\n",
    "        precisionK = []\n",
    "        sortedInd = sortedInd[::-1]\n",
    "        N = train_graph_data.N\n",
    "        for ind in sortedInd:\n",
    "            x = ind / N\n",
    "            y = ind % N\n",
    "            if (x == y or train_graph_data.adj_matrix[x].toarray()[0][y] == 1):\n",
    "                continue \n",
    "            count += 1\n",
    "            if (origin_graph_data.adj_matrix[x].toarray()[0][y] == 1):\n",
    "                cur += 1\n",
    "            precisionK.append(1.0 * cur / count)\n",
    "            if count > max_index:\n",
    "                break\n",
    "        return precisionK\n",
    "    precisionK = get_precisionK(embedding, train_graph_data, origin_graph_data, np.max(check_index))\n",
    "    ret = []\n",
    "    for index in check_index:\n",
    "        print( \"precisonK[%d] %.2f\" % (index, precisionK[index - 1]))\n",
    "        ret.append(precisionK[index - 1])\n",
    "    return ret\n",
    " \n",
    "\n",
    "def check_multi_label_classification(X, Y, test_ratio = 0.9):\n",
    "    def small_trick(y_test, y_pred):\n",
    "        y_pred_new = np.zeros(y_pred.shape,np.bool)\n",
    "        sort_index = np.flip(np.argsort(y_pred, axis = 1), 1)\n",
    "        for i in range(y_test.shape[0]):\n",
    "            num = sum(y_test[i])\n",
    "            for j in range(num):\n",
    "                y_pred_new[i][sort_index[i][j]] = True\n",
    "        return y_pred_new\n",
    "        \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = test_ratio)\n",
    "    clf = OneVsRestClassifier(LogisticRegression())\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict_proba(x_test)\n",
    "    \n",
    "    ## small trick : we assume that we know how many label to predict\n",
    "    y_pred = small_trick(y_test, y_pred)\n",
    "    \n",
    "    micro = f1_score(y_test, y_pred, average = \"micro\")\n",
    "    macro = f1_score(y_test, y_pred, average = \"macro\")\n",
    "    return (\"micro_f1: %.4f macro_f1 : %.4f\" % (micro, macro))\n",
    "    #############################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
